{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['key', 'x', 'y', 'alpha', 'beta', 'target']\n",
      "Numeric Features: ['x', 'y']\n",
      "Categorical Features: ['alpha', 'beta']\n",
      "Target: target\n",
      "Unused Features: ['key']\n",
      "Model directory: trained_models/reg-model-1_2\n",
      "Hyper-parameters: [('dropout_prob', 0.0), ('hidden_units', [8, 4])]\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/reg-model-1_2', '_tf_random_seed': 19830610, '_save_summary_steps': 100, '_save_checkpoints_steps': 480, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000000032732400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "\n",
      "Estimator Type: <class 'tensorflow.python.estimator.canned.dnn.DNNRegressor'>\n",
      "\n",
      "Resuming training...\n",
      "Experiment started at 14:21:27\n",
      ".......................................\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 480 or save_checkpoints_secs None.\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From D:\\ProgramLanguageCore\\Python\\anaconda351\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into trained_models/reg-model-1_2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 178359.03, step = 1\n",
      "INFO:tensorflow:global_step/sec: 52.91\n",
      "INFO:tensorflow:loss = 151433.4, step = 101 (1.892 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4934\n",
      "INFO:tensorflow:loss = 154375.05, step = 201 (1.904 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 240 into trained_models/reg-model-1_2\\model.ckpt.\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-24-14:21:34\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-24-14:21:34\n",
      "INFO:tensorflow:Saving dict for global step 240: average_loss = 310.65808, global_step = 240, label/mean = 1.4932353, loss = 155329.05, prediction/mean = 1.2253644\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 240: trained_models/reg-model-1_2\\model.ckpt-240\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'DecodeCSV:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>, <tf.Tensor 'Square:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Square_1:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Mul:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Sqrt:0' shape=(?, 1) dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "WARNING:tensorflow:From D:\\ProgramLanguageCore\\Python\\anaconda351\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/reg-model-1_2\\export\\estimate\\temp-b'1543069295'\\saved_model.pbtxt\n",
      "INFO:tensorflow:Loss for final step: 142987.17.\n",
      ".......................................\n",
      "Experiment finished at 14:21:35\n",
      "\n",
      "Experiment elapsed time: 8.278 seconds\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-24-14:21:36\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-24-14:21:36\n",
      "INFO:tensorflow:Saving dict for global step 240: average_loss = 299.08524, global_step = 240, label/mean = 1.7656369, loss = 149542.62, prediction/mean = 1.1337944\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 240: trained_models/reg-model-1_2\\model.ckpt-240\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-24-14:21:36\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-24-14:21:37\n",
      "INFO:tensorflow:Saving dict for global step 240: average_loss = 346.94647, global_step = 240, label/mean = 1.2374389, loss = 173473.23, prediction/mean = 1.2355554\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 240: trained_models/reg-model-1_2\\model.ckpt-240\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-24-14:21:37\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-24-14:21:38\n",
      "INFO:tensorflow:Saving dict for global step 240: average_loss = 298.26282, global_step = 240, label/mean = 0.38619202, loss = 149131.4, prediction/mean = 1.1322839\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 240: trained_models/reg-model-1_2\\model.ckpt-240\n",
      "############################################################################################\n",
      "############################################################################################\n",
      "parse_csv_row-->  dict_keys(['x', 'y', 'alpha', 'beta']) dict_values([<tf.Tensor 'DecodeCSV:1' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=() dtype=float32>, <tf.Tensor 'DecodeCSV:3' shape=() dtype=string>, <tf.Tensor 'DecodeCSV:4' shape=() dtype=string>])\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'arg2:0' shape=() dtype=float32>, <tf.Tensor 'arg3:0' shape=() dtype=float32>, <tf.Tensor 'arg0:0' shape=() dtype=string>, <tf.Tensor 'arg1:0' shape=() dtype=string>, <tf.Tensor 'Square:0' shape=() dtype=float32>, <tf.Tensor 'Square_1:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=() dtype=float32>, <tf.Tensor 'Sqrt:0' shape=() dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predictions': array([4.101386], dtype=float32)}\n",
      "{'predictions': array([-1.3655725], dtype=float32)}\n",
      "{'predictions': array([2.1271155], dtype=float32)}\n",
      "{'predictions': array([1.7602656], dtype=float32)}\n",
      "{'predictions': array([1.4298067], dtype=float32)}\n",
      "{'predictions': array([2.0573232], dtype=float32)}\n",
      "process_features-->  dict_keys(['x', 'y', 'alpha', 'beta', 'x_2', 'y_2', 'xy', 'dist_xy']) dict_values([<tf.Tensor 'DecodeCSV:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'DecodeCSV:1' shape=(?, 1) dtype=float32>, <tf.Tensor 'DecodeCSV:2' shape=(?, 1) dtype=string>, <tf.Tensor 'DecodeCSV:3' shape=(?, 1) dtype=string>, <tf.Tensor 'Square:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Square_1:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Mul:0' shape=(?, 1) dtype=float32>, <tf.Tensor 'Sqrt:0' shape=(?, 1) dtype=float32>])\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: ['serving_default', 'regression']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2\\model.ckpt-240\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/reg-model-1_2/my_export\\temp-b'1543069298'\\saved_model.pbtxt\n",
      "trained_models/reg-model-1_2/my_export\n",
      "trained_models/reg-model-1_2/my_export/1543069298\n",
      "INFO:tensorflow:Restoring parameters from trained_models/reg-model-1_2/my_export/1543069298\\variables\\variables\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got unexpected keys in input_dict: {'csv_rows'}\nexpected: set()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8a082d1c9d0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m     \u001b[1;31m#main(None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramLanguageCore\\Python\\anaconda351\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8a082d1c9d0a>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(argv)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mExperimentConfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[0mexport_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexport_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMetaMD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMODEL_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'/my_export'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m     \u001b[0mpredict_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8a082d1c9d0a>\u001b[0m in \u001b[0;36mpredict_input\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[0mpredictor_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msaved_model_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature_def_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"prediction\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictor_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'csv_rows'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"0.5,1,ax01,bx02\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"-0.5,-1,ax02,bx02\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramLanguageCore\\Python\\anaconda351\\lib\\site-packages\\tensorflow\\contrib\\predictor\\predictor.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, input_dict)\u001b[0m\n\u001b[0;32m     68\u001b[0m       raise ValueError(\n\u001b[0;32m     69\u001b[0m           'Got unexpected keys in input_dict: {}\\nexpected: {}'.format(\n\u001b[1;32m---> 70\u001b[1;33m               unexpected_keys, expected_keys))\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Got unexpected keys in input_dict: {'csv_rows'}\nexpected: set()"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil,os,path\n",
    "import math\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "from typing import ByteString, Dict, Tuple, Optional, Any,List\n",
    "\n",
    "\n",
    "class MetaPoj(SimpleNamespace):\n",
    "    RESUME_TRAINING = False\n",
    "    PROCESS_FEATURES = True\n",
    "    MULTI_THREADING = True\n",
    "\n",
    "\n",
    "class MetaRaw(SimpleNamespace):\n",
    "    root_p = 'E:/my_proj/ml_proj/sk_sp_tf_ks/my_tf/'\n",
    "    TRAIN_DATA_FILES_PATTERN = root_p + 'data/reg/train-*.csv'\n",
    "    VALID_DATA_FILES_PATTERN = root_p + 'data/reg/valid-*.csv'\n",
    "    TEST_DATA_FILES_PATTERN = root_p + 'data/reg/test-*.csv'\n",
    "\n",
    "    HEADER = ['key', 'x', 'y', 'alpha', 'beta', 'target']\n",
    "    HEADER_DEFAULTS = [[0], [0.0], [0.0], ['NA'], ['NA'], [0.0]]\n",
    "    NUMERIC_FEATURE_NAMES = ['x', 'y']\n",
    "    CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY = {'alpha': ['ax01', 'ax02'], 'beta': ['bx01', 'bx02']}\n",
    "    CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.keys())\n",
    "    FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "    TARGET_NAME = 'target'\n",
    "    UNUSED_FEATURE_NAMES = list(set(HEADER) - set(FEATURE_NAMES) - {TARGET_NAME})\n",
    "    print(\"Header: {}\".format(HEADER))\n",
    "    print(\"Numeric Features: {}\".format(NUMERIC_FEATURE_NAMES))\n",
    "    print(\"Categorical Features: {}\".format(CATEGORICAL_FEATURE_NAMES))\n",
    "    print(\"Target: {}\".format(TARGET_NAME))\n",
    "    print(\"Unused Features: {}\".format(UNUSED_FEATURE_NAMES))\n",
    "\n",
    "\n",
    "class MetaETL(SimpleNamespace):\n",
    "    CONSTRUCTED_NUMERIC_FEATURES_NAMES = ['x_2', 'y_2', 'xy', 'dist_xy']\n",
    "    ALL_NUMERIC_FEATURE_NAMES = MetaRaw.NUMERIC_FEATURE_NAMES + CONSTRUCTED_NUMERIC_FEATURES_NAMES\n",
    "\n",
    "\n",
    "def parse_csv_row(csv_row: ByteString)->Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n",
    "    \"\"\"get a string tensor\"\"\"\n",
    "    columns = tf.decode_csv(csv_row, record_defaults=MetaRaw.HEADER_DEFAULTS)\n",
    "    features: Dict[str, tf.Tensor] = dict(zip(MetaRaw.HEADER, columns))\n",
    "    for column in MetaRaw.UNUSED_FEATURE_NAMES:\n",
    "        features.pop(column)\n",
    "    target = features.pop(MetaRaw.TARGET_NAME)\n",
    "    print('parse_csv_row--> ', features.keys(),features.values())\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def process_features(features: Dict[str, tf.Tensor])->Dict[str, tf.Tensor]:\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    features[\"xy\"] = tf.multiply(features['x'], features['y'])  # features['x'] * features['y']\n",
    "    features['dist_xy'] = tf.sqrt(tf.squared_difference(features['x'], features['y']))\n",
    "    print('process_features--> ', features.keys(), features.values())\n",
    "    return features\n",
    "\n",
    "\n",
    "def csv_input_fn(file_name_pattern: str, mode: str=tf.estimator.ModeKeys.EVAL, batch_size: int=200,\n",
    "                 num_epochs: Optional[int]=None, skip_header_lines: int=0)->Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    input_file_names: tf.Tensor = tf.matching_files(pattern=file_name_pattern)\n",
    "    dataset = tf.data.TextLineDataset(input_file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    dataset = dataset.map(parse_csv_row)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    if MetaPoj.PROCESS_FEATURES:\n",
    "        dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    feaures, target = iterator.get_next()\n",
    "    return feaures, target\n",
    "\n",
    "\n",
    "def get_feature_columns()->Dict[str, Any]:\n",
    "    # 将各种列赋予类型，根据类型可以有响应处理\n",
    "    numeric_columns: Dict[str, Any] = {feature_name: tf.feature_column.numeric_column(feature_name) for\n",
    "                                       feature_name in MetaETL.ALL_NUMERIC_FEATURE_NAMES}\n",
    "\n",
    "    categorical_column_with_vocabulary: Dict[str, Any] = {\n",
    "        item[0]: tf.feature_column.categorical_column_with_vocabulary_list(item[0], item[1])\n",
    "        for item in MetaRaw.CATEGORICAL_FEATURE_NAMES_WITH_VOCABULARY.items()}\n",
    "\n",
    "    feature_columns = {}\n",
    "    if numeric_columns is not None:\n",
    "        feature_columns.update(numeric_columns)\n",
    "\n",
    "    if categorical_column_with_vocabulary is not None:\n",
    "        feature_columns.update(categorical_column_with_vocabulary)\n",
    "    # 之所用用字典，方便后面按照名称索引进一步处理，因为列还没有进一步转化\n",
    "    feature_columns['alpha_X_beta'] = tf.feature_column.crossed_column([feature_columns['alpha'], feature_columns['beta']], 4)\n",
    "    return feature_columns\n",
    "\n",
    "\n",
    "def get_final_feature_columns()->List[Any]:\n",
    "    # 检查字段并对字段做按类别做出进一步处理\n",
    "    FEATURE_COLUMNS = list(get_feature_columns().values())\n",
    "    dense_columns = list(filter(lambda column: isinstance(column, feature_column._NumericColumn),FEATURE_COLUMNS))\n",
    "\n",
    "    categorical_columns = list(\n",
    "        filter(lambda column: isinstance(column, feature_column._VocabularyListCategoricalColumn) |\n",
    "                              isinstance(column, feature_column._BucketizedColumn), FEATURE_COLUMNS))\n",
    "\n",
    "    # convert categorical columns to indicators,独热码化\n",
    "    indicator_columns = list(map(lambda column: tf.feature_column.indicator_column(column), categorical_columns))\n",
    "    return dense_columns + indicator_columns\n",
    "\n",
    "\n",
    "def create_estimator(run_config, hparams)->tf.estimator.Estimator:\n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        feature_columns=get_final_feature_columns(),\n",
    "        hidden_units=hparams.hidden_units,\n",
    "        dropout=hparams.dropout_prob,\n",
    "        activation_fn=tf.nn.elu,\n",
    "        optimizer=tf.train.AdamOptimizer(),\n",
    "        config=run_config)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Estimator Type: {}\".format(type(estimator)))\n",
    "    print(\"\")\n",
    "    return estimator\n",
    "\n",
    "# ##########################准备试验数据#############################\n",
    "\n",
    "\n",
    "#  #####定义试验参数#######\n",
    "class MetaMD(SimpleNamespace):\n",
    "    EVAL_AFTER_SEC = 15\n",
    "    NUM_EPOCHS = 10\n",
    "    BATCH_SIZE = 500\n",
    "    TRAIN_SIZE = 12000\n",
    "    TOTAL_STEPS = (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS\n",
    "\n",
    "    MODEL_NAME = 'reg-model-1_2'\n",
    "    MODEL_DIR = path.Path('trained_models/{}'.format(MODEL_NAME)).makedirs_p()\n",
    "    EXPORT_DIR = path.Path(MODEL_DIR + \"/export/estimate\").makedirs_p()\n",
    "\n",
    "    HPARAMS = tf.contrib.training.HParams(hidden_units=[8, 4], dropout_prob=0.0)\n",
    "\n",
    "    RUN_CONFIG = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps=480, # to evaluate after each 20 epochs => (12000/500) * 20\n",
    "        tf_random_seed=19830610,\n",
    "        model_dir=MODEL_DIR)\n",
    "\n",
    "    print(\"Model directory: {}\".format(RUN_CONFIG.model_dir))\n",
    "    print(\"Hyper-parameters: {}\".format(HPARAMS))\n",
    "\n",
    "\n",
    "#  #####定义服务函数，用于配置导出 #######\n",
    "def csv_serving_input_fn()->tf.estimator.export.ServingInputReceiver:\n",
    "    SERVING_HEADER = ['x', 'y', 'alpha', 'beta']\n",
    "    SERVING_HEADER_DEFAULTS = [[0.0], [0.0], ['NA'], ['NA']]\n",
    "    rows_string_tensor = tf.placeholder(dtype=tf.string, shape=[None], name='csv_rows')\n",
    "\n",
    "    receiver_tensor = {'csv_rows': rows_string_tensor}\n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=SERVING_HEADER_DEFAULTS)\n",
    "    features = dict(zip(SERVING_HEADER, columns))\n",
    "\n",
    "    if MetaPoj.PROCESS_FEATURES:\n",
    "        features = process_features(features)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)\n",
    "\n",
    "\n",
    "# 定义训练和验证的输入函数\n",
    "class ExperimentConfig(SimpleNamespace):\n",
    "    train_input_tr_fn = lambda: csv_input_fn(\n",
    "        file_name_pattern=MetaRaw.TRAIN_DATA_FILES_PATTERN, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "        num_epochs=MetaMD.NUM_EPOCHS, batch_size=MetaMD.BATCH_SIZE)\n",
    "\n",
    "    eval_input_tr_fn = lambda: csv_input_fn(\n",
    "        file_name_pattern=MetaRaw.VALID_DATA_FILES_PATTERN, mode=tf.estimator.ModeKeys.EVAL,\n",
    "        num_epochs=1, batch_size=MetaMD.BATCH_SIZE)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_tr_fn, max_steps=MetaMD.TOTAL_STEPS)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_tr_fn,\n",
    "        exporters=[tf.estimator.LatestExporter(name=\"estimate\",serving_input_receiver_fn=csv_serving_input_fn,exports_to_keep=1,as_text=True)],\n",
    "        steps=None, throttle_secs=MetaMD.EVAL_AFTER_SEC)  # evalute after each 15 training seconds!\n",
    "\n",
    "    train_input_eval_fn = lambda: csv_input_fn(file_name_pattern=MetaRaw.TRAIN_DATA_FILES_PATTERN,\n",
    "        mode=tf.estimator.ModeKeys.EVAL, num_epochs=MetaMD.NUM_EPOCHS, batch_size=MetaMD.BATCH_SIZE)\n",
    "\n",
    "    eval_input_eval_fn = lambda: csv_input_fn(file_name_pattern=MetaRaw.VALID_DATA_FILES_PATTERN,\n",
    "        mode=tf.estimator.ModeKeys.EVAL, num_epochs=1, batch_size=MetaMD.BATCH_SIZE)\n",
    "\n",
    "    test_input_eval_fn = lambda: csv_input_fn(file_name_pattern=MetaRaw.TEST_DATA_FILES_PATTERN,\n",
    "                                              mode=tf.estimator.ModeKeys.EVAL, num_epochs=1,\n",
    "                                              batch_size=MetaMD.BATCH_SIZE)\n",
    "\n",
    "\n",
    "def run_experiment(EXP_CFG, estimator):\n",
    "    if MetaPoj.RESUME_TRAINING:\n",
    "        print(\"Removing previous artifacts...\")\n",
    "        shutil.rmtree(MetaMD.MODEL_DIR, ignore_errors=True)\n",
    "    else:\n",
    "        print(\"Resuming training...\")\n",
    "\n",
    "    time_start = datetime.utcnow()\n",
    "    print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "    print(\".......................................\")\n",
    "    tf.estimator.train_and_evaluate(estimator=estimator, train_spec=EXP_CFG.train_spec, eval_spec=EXP_CFG.eval_spec)\n",
    "    time_end = datetime.utcnow()\n",
    "    print(\".......................................\")\n",
    "    print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "    print(\"\")\n",
    "    time_elapsed = time_end - time_start\n",
    "    print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    #####################################################################################################\n",
    "    import math\n",
    "    train_results = estimator.evaluate(input_fn=EXP_CFG.train_input_eval_fn, steps=1)\n",
    "    #train_rmse = round(math.sqrt(train_results[\"average_loss\"]), 5)\n",
    "    print(\"############################################################################################\")\n",
    "    #print(\"# Train RMSE: {} - {}\".format(train_rmse, train_results))\n",
    "    print(\"############################################################################################\")\n",
    "\n",
    "    eval_results = estimator.evaluate(input_fn=EXP_CFG.eval_input_eval_fn, steps=1)\n",
    "    #eval_rmse = round(math.sqrt(eval_results[\"average_loss\"]), 5)\n",
    "    print(\"############################################################################################\")\n",
    "    #print(\"# eval RMSE: {} - {}\".format(eval_rmse, eval_results))\n",
    "    print(\"############################################################################################\")\n",
    "\n",
    "    test_results = estimator.evaluate(input_fn=EXP_CFG.test_input_eval_fn, steps=1)\n",
    "    #test_rmse = round(math.sqrt(test_results[\"average_loss\"]), 5)\n",
    "    print(\"############################################################################################\")\n",
    "    #print(\"# Test RMSE: {} - {}\".format(test_rmse, test_results))\n",
    "    print(\"############################################################################################\")\n",
    "    predictions = estimator.predict(input_fn=EXP_CFG.test_input_eval_fn)\n",
    "    for it in range(6): print(next(predictions))\n",
    "\n",
    "\n",
    "def export_model(estimator, model_dir, sub_dir=''):\n",
    "    export_dir = path.Path(model_dir + sub_dir).makedirs_p()\n",
    "    estimator.export_savedmodel(export_dir_base=export_dir, serving_input_receiver_fn=csv_serving_input_fn, as_text=True)\n",
    "    print(export_dir)\n",
    "    return export_dir\n",
    "\n",
    "\n",
    "def predict_input(export_dir):\n",
    "    saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1]\n",
    "    print(saved_model_dir)\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(export_dir=saved_model_dir, signature_def_key=\"prediction\")\n",
    "    output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "    print(output)\n",
    "\n",
    "\n",
    "def predict_input2(model_dir):\n",
    "    saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1]\n",
    "    print(saved_model_dir)\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(export_dir=saved_model_dir,signature_def_key=\"predict\")\n",
    "    output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "    print(output)\n",
    "\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    estimator = create_estimator(MetaMD.RUN_CONFIG, MetaMD.HPARAMS)\n",
    "    run_experiment(ExperimentConfig, estimator)\n",
    "    export_dir = export_model(estimator, MetaMD.MODEL_DIR, sub_dir='/my_export')\n",
    "    predict_input(export_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)\n",
    "    #main(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\n",
      "INFO:tensorflow:Restoring parameters from E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\\variables\\variables\n",
      "{'predictions': array([[61.30815 ],\n",
      "       [-6.153293]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def predict_input2(model_dir):\n",
    "    export_dir = model_dir +\"/export/estimate/\"\n",
    "    saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1] \n",
    "    print(saved_model_dir)\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=saved_model_dir,\n",
    "        signature_def_key=\"predict\")\n",
    "\n",
    "    output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "    print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_input2(model_dir):\n",
    "    saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1]\n",
    "    print(saved_model_dir)\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(export_dir=saved_model_dir,signature_def_key=\"predict\")\n",
    "    output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "    print(output)\n",
    "    \n",
    "def predict_input(export_dir):\n",
    "    saved_model_dir = export_dir + \"/\" + os.listdir(path=export_dir)[-1]\n",
    "    print(saved_model_dir)\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(export_dir=saved_model_dir, signature_def_key=\"predict\")\n",
    "    output = predictor_fn({'csv_rows': [\"0.5,1,ax01,bx02\", \"-0.5,-1,ax02,bx02\"]})\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\n",
      "INFO:tensorflow:Restoring parameters from E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\\variables\\variables\n",
      "{'predictions': array([[61.30815 ],\n",
      "       [-6.153293]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "model_dir = r'E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2'\n",
    "export_dir = model_dir +\"/export/estimate/\"\n",
    "predict_input2(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\n",
      "INFO:tensorflow:Restoring parameters from E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2/export/estimate//1543068376\\variables\\variables\n",
      "{'predictions': array([[61.30815 ],\n",
      "       [-6.153293]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "model_dir = r'E:\\my_proj\\ml_proj\\sk_sp_tf_ks\\my_tf\\estimator\\trained_models\\reg-model-1_2'\n",
    "export_dir = model_dir +\"/export/estimate/\"\n",
    "predict_input(export_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
